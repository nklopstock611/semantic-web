# Semantic Web Project

This project is divided in three main tasks.
- **Task 1**: Get academic paper related data from different data sources and extract all the metadata possible.
- **Task 2**: Create a knowledge graph with the data extracted in Task 1.
- **Task 3**: Create a web application that allows users to query the knowledge graph created in Task 2 and get specific PDFs.

## Task 1
We divided this task in two subtasks:
- **Subtask 1.1**: Get academic paper related data from different data sources (i.e. Academic Scholar API).
- **Subtask 1.2**: Extract all the metadata possible from the data obtained in Subtask 1.1. using tools like GROBID.

### Subtask 1.1
We used the Academic Scholar API to get academic paper related data. We used the following query to get the data.

To run the code, first you need to install the required libraries. You can do this by running the following command:

```pip install -r requirements.py``` 

This will install all the required libraries for all the tasks.

After installing the required libraries, you can start by going into the `/semantic-web/first-task/pdf-downloader/` directory. There, you'll find four main python scripts:
- `main.py`: Runs all the code in the correct order.
- `requester.py`: Does all the requests to the Academic Scholar API.
- `logic.py`: Has the logic to verify the existence of papers and its attributes.
- `utils.py`: We'll use this script in the second task.

To run the code, you can use the following command:

```
cd first-task/pdf-downloader
python main.py
```

And it will start doing the requests and downloading the PDFs. Important to note: at the beginnig of each file, there will be some variables with specific paths. You can change them to your specific paths.

### Subtask 1.2
Now that we have a big number of PDFs, we can start extracting metadata from them. We used GROBID to extract the metadata.

To run the code, first you need to install scipdf_parser and the model. You can do this by running the following commands:

```
pip install git+https://github.com/titipata/scipdf_parser
python -m spacy download en_core_web_sm
```

Now, we have to start the GROBID server. You can do this by running the following command:

```
docker-compose up -d
```

With the server up and running, we can run the actual code!

In the `pdf-info-extractor` directory, you'll find the following files:
- `main.py`: Runs all the code in the correct order.
- `xml_analyzer.py`: Analyzes the XML files generated by GROBID.
- `extractor.py`: Creates the JSON file with the metadata extracted.
- `webscraper.py`: Automation of what you would do manually in the GROBID website. (We do not use this file. It was part of the starting test-phase of the project. We left it to show the evolution of the project)

To run the code, you can use the following command:

```
cd first-task/pdf-downloader
python main.py
```

And it will start analyzing the XML files and creating the JSON file with the metadata extracted. Important to note: at the beginnig of each file, there will be some variables with specific paths. You can change them to your specific paths.

## Task 2
We divided this task in three subtasks:
- **Subtask 2.1**: Adjust the metadata from the previous task.
- **Subtask 2.2**: Use TextRazor to get the entities and keywords from each text in each paper.
- **Subtask 2.3**: Model all classes and properties, and instances.

### Subtask 2.1
After presenting the first task, we realized we weren't sure of the number of referenced papers we had downloaded. So, we implemented a quick function that get a list of every referenced paper from each paper in the JSON and does a request for its PDF (if able, it downloads it).

You can find that code in `/semantic-web/first-task/pdf-downloader/utils.py`.

With this process, we managed to get more or less 2000 more PDFs.

### Subtask 2.2
With a more complete metadata, the next step was using tools like TextRazor to get keywords from the introduction, abstract and conclusion of each paper.

If you go into the `/semantic-web/second-task/text-analytics/` directory, you'll find three files:

- `main.py`: Runs all the code in the correct order.
- `text_analytics.py`: Gets the keywords and entities for each paper and updates the JSON metadata file.
- `utis.py`: File with data cleaning processes.

To run the code, you can use the following command:

```
cd second-task/text-analytics
python main.py
```

And the requests to TextRazor will begin. This will update the JSON metadata file. The data cleaning processes must be executed next with the following command:

```
python utils.py
```

### Subtask 2.3
Lastly, we have to model all the classes and properties to fully duplicate the natural rules of the metadata we collected.

You can find the ontology RDF file in `/semantic-web/second-task/rdf-model/rdf_definition.ipynb`. In that file, you'll find the ontology model, the instances and some SPARQL and CYPHER (neo4j) queries. We recommend to download the RDF files (`/semantic-web/second-task/rdf-model/rdf-files`) and not no fully execute the Notebook. This, because the OWL inference closure function after loading the instances can take 60 minutes to finish.

In the `/semantic-web/second-task/rdf-model/semantic_instances_only.txt` file there's a link to a Google Drive folder. There, you'll the find JSON with all the metadata, both .rdf files (ontology and instances) and the neo4j database dump.


# DRAFT
Para correr el front:
```
npm install
cd third-task/front
npm start
```

Para correr el back:
```
cd third-task/api
uvicorn main:app --reload
```